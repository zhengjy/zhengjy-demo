kafka:
​	kafka是一种分布式的，基于发布/订阅的消息系统。kafka对消息保存时根据Topic进行分类，发送消息者成为Producer，消息接受者为Consumer，此为kafka集群有多个kafka实例组成，
​	每个实例(server)成为broker,无论是kafka集群，还是producer和consumer都依赖zookeeper来保证系统可用性集群保存一些meta信息。
​		主要设计目标为：
​		1)以时间复杂度为O(1)的方式提供消息持久化功能，即使对TB以上的数据也能保证常数(数值不变的常量)时间复杂度的访问
​		2)高吞吐率，即使在非常廉价的机器也能做到单机支持每秒100K条以上消息传输
​		3)支持kafka Server间的消息分区，及分布式消费，同时保证每个Partition内的消息顺序传输
​		4)同时支持离线数据处理和实时数据处理

	topics：每条发布到Kafka集群的消息都有一个类别，这个类别被称为Topic。一个topics可以认为是一类消息，每个topics将被分为多个partition(区),
	每个partition在存储层面是append log文件。任何发布到此parition的消息都会被直接追加到log文件
	的尾部，每条消息在文件中的位置成为offset(偏移量),offset为一个long型数字，他是唯一标记一条信息。kafka并没有提供其他额外的索引机制来存储offset,因为kafka中几乎不允许
	对消息进行”随机读写“.


dubbo远程调用协议：
​	REST(HTTP+JSON/XML) 性能高
​	RPS(TCP+Hessian 2进制序列化) 默认

启动提供者注册zk,消费者订阅服务，消费者，建立链接，调用接口，将参数序列化传给提供者，提供者反序列化代理执行调用方法，在序列化出去传给消费者，消费者在反序列化。	
​	
本地暴露和远程暴露	
​	在dubbo中我们一个服务可能既是Provider,又是Consumer,因此就存在他自己调用自己服务的情况,如果再通过网络去访问,那自然是舍近求远,因此他是有本地暴露服务的这个设计.
​	本地暴露是暴露在JVM中,不需要网络通信.
​	远程暴露是将ip,端口等信息暴露给远程客户端,调用时需要网络通信.
​	
dubbo协议：
​	dubbo缺省协议采用单一长链接和NIO异步通讯，适用于小数据量大并发的调用，以及服务消费者机器数远大于服务提供者机器数的情况。
​	缺省协议，使用基于mina1.1.7+hessian3.2.1的tbremoting交互。
​		连接个数：单连接
​		连接方式：长连接
​		传输协议：TCP
​		传输方式：NIO异步传输
​		序列化：Hessian二进制序列化
​		适用范围：传入传出参数数据包较小（建议小于100K），消费者比提供者个数多，单一消费者无法压满提供者，尽量不要用dubbo协议传输大文件或超大字符串。
​		适用场景：常规远程服务方法调用
RMI协议：
​	rmi协议采用JDK标准的java.rmi.*实现，采用阻塞式短连接和JDK标准序列化方式
​	Java标准的远程调用协议。
​		连接个数：多连接
​		连接方式：短连接
​		传输协议：TCP
​		传输方式：同步传输
​		序列化：Java标准二进制序列化
​		适用范围：传入传出参数数据包大小混合，消费者与提供者个数差不多，可传文件。
​		适用场景：常规远程服务方法调用，与原生RMI服务互操作
hessian协议：
​	Hessian协议用于集成Hessian的服务，Hessian底层采用Http通讯，采用Servlet暴露服务，Dubbo缺省内嵌Jetty作为服务器实现
​	基于Hessian的远程调用协议。
​		连接个数：多连接
​		连接方式：短连接
​		传输协议：HTTP
​		传输方式：同步传输
​		序列化：Hessian二进制序列化
​		适用范围：传入传出参数数据包较大，提供者比消费者个数多，提供者压力较大，可传文件。
​		适用场景：页面传输，文件传输，或与原生hessian服务互操作
http协议:
​	采用Spring的HttpInvoker实现
​	基于http表单的远程调用协议。
​		连接个数：多连接
​		连接方式：短连接
​		传输协议：HTTP
​		传输方式：同步传输
​		序列化：表单序列化（JSON）
​		适用范围：传入传出参数数据包大小混合，提供者比消费者个数多，可用浏览器查看，可用表单或URL传入参数，暂不支持传文件。
​		适用场景：需同时给应用程序和浏览器JS使用的服务。

dubbo节点：
​	Provider：暴露服务的服务提供方。
​	Consumer：调用远程服务的服务消费方。
​	Register：服务注册与发现的注册中心。
​	Monitor：统计服务调用次调和调用时间的监控中心。
​	Container：服务运行容器。

dubbo调用关系：
​	1、服务器启动容器负责启动、加载、运行服务提供者。
​	2、服务提供者在启动时，向注册中心注册自己提供的服务。
​	3、服务消费者在启动时，向注册中心订阅自己所需的服务。
​	4、注册中心返回提供者地址列表给消费者，如有变更，注册中心将基于长连接推送变更数据给消费者。
​	5、服务消费者，从提供者地址列表中，基于软件负载均衡算法，选一台提供者进行调用，如果调用失败，在调用另一台调用。
​	6、服务消费者和提供者，在内存中累计调用次数和调用时间，定时每分钟发送一次统计数据到监控中心。
​	服务提供方发布服务到服务注册中心
​	服务消费方从服务注册中心订阅服务
​	服务消费方调用已注册的可用服务

dubbo应用执行流程：
​	1、服务提供者启动，根据协议信息绑定到配置的IP和端口上，如果已有服务绑定相同的IP和端口则跳过
​	2、注册服务信息至注册中心
​	3、客户端启动，根据接口和协议信息订阅注册中心订阅中注册的服务，注册中心将存活的服务地址通知到客户端，当有服务信息变更时客户端可以通过定时
通知得到变更信息
​	4、在客户端需要调用服务时，从内存中拿到上次通知的所有存活地址，根据路由信息和负载均衡选择最终调用的服务地址，发起调用
​	5、通过filter分别在客户端发送请求前和服务端收到请求后，通过异步记录一些需要的信息传递到monitor做监控

dubbo负载均衡：
​	1、Random,随机,按权重配置随机访问概率，调用量越大分配越均匀，默认是这种方式。
​	2、RoundRobin，轮询，按权重设置轮询比例，如果存在较慢的机器容易在这台机器的请求阻塞越多
​	3、LeastActive，最少活跃调用数，不支持权重，只能根据自动识别的活跃数分配，不能灵活调配
​	4、ConsistentHash，一致性hash,对相同参数的请求路由到一个服务提供者上，如果有类似灰度发布需求可采用
​	dubbo的负载均衡机制是在客户端调用时通过内存中的服务信息及配置的负载均衡策略选择，如果对自己的系统没有一个全面的认知，建议采用默认random

其他了解：
​	1.<dubbo:service/> 服务配置，用于暴露一个服务，定义服务的元信息，一个服务可以用多个协议暴露，一个服务也可以注册到多个注册中心。
​	2.可通过注解提供服务和调用。
​	3.Dubbo缺省会在启动时检查依赖的服务是否可用，不可用时会抛出异常，阻止Spring初始化完成，以便上线时，能及早发现问题，默认check=true;
​		关闭所有服务的启动时检查：(没有提供者时报错)<dubbo:consumer check="false" />
​	4.在开发及测试环境下，经常需要绕过注册中心，只测试指定服务提供者，这时候可能需要点对点直连。自动加载${user.home}/dubbo-resolve.properties文件，不需要配置
​	5.可以让服务提供者开发方，只订阅服务(开发的服务可能依赖其它服务)，而不注册正在开发的服务，通过直连测试正在开发的服务。
​		<dubbo:registry address="10.20.153.10:9090" register="false" />
​	6.多协议暴露服务
​		<dubbo:protocol name="dubbo" port="20880" />
​		<dubbo:protocol name="hessian" port="8080" />
​	7.多注册中心注册
​		<dubbo:registry id="hangzhouRegistry" address="10.20.141.150:9090" />
​		<dubbo:registry id="qingdaoRegistry" address="10.20.141.151:9010" default="false" />
dubbo过滤器：


1、注册标签解析器（自定义的dubbo标签），生成对应的BeanDefinition交给spring管理
2、验证所需要的组件是否已经准备好如(consumer、provider)

dubbo源码解析：
​	使用dubbo.xsd文件来定义dubbo相关的元素及属性;DubboNamespaceHanndler来像spring容器注册dubbo的元素解析器;DubboBeanDefinitionParser用来解析具体的dubbo元素。
​	
​	服务提供者暴露一个服务的详细过程:
​		再ServiceConfig.export()或ReferenceConfig.get()初始化时，将Bean对象转换URL格式，所有Bean熟悉转换URL的参数。然后将URL传给
​		协议扩展点，基于扩展点的扩展点自适应机制，根据URL的协议头，进行不同的服务暴露或引用。
​		
​		首先ServiceConfig类拿到对外提供服务的实际类ref(如：HelloWorldImpl)，然后通过ProxyFactory类getInvoker方法使用ref生成一个
​		AbstractProxyInvoker实例，到这一步就完成具体服务到Invoker的转换。接下来就是Invoker转到Exporter的过程。Dubbo协议的Invoker
​		转为Exporter发生再DubboProtocol类的exportf方法，它主要是打开socket监听服务，并接收客户端发来的各种请求。
​	
​	服务消费者消费一个服务的详细过程：
​		首先ReferenceConfig类的init方法调用Protocol的refer方法生成Invoker实例，接下来Invoker转换位客户端需要的接口如(HellpWorld)

dubbo设计@adaptive注解的原因
​	为什么要设计adaptive？注解在类上和注解在方法上的区别？
​	adaptive设计的目的是为了识别固定已知类和扩展未知类。
​	1.注解在类上：代表人工实现，实现一个装饰类（设计模式中的装饰模式），它主要作用于固定已知类，
​	目前整个系统只有2个，AdaptiveCompiler、AdaptiveExtensionFactory。
​	a.为什么AdaptiveCompiler这个类是固定已知的？因为整个框架仅支持Javassist和JdkCompiler。
​	a.为什么AdaptiveExtensionFactory这个类是固定已知的？因为整个框架仅支持2个objFactory,一个是spi,另一个是spring
​	2.注解在方法上：代表自动生成和编译一个动态的Adpative类，它主要是用于SPI，因为spi的类是不固定、未知的扩展类，所以设计了动态$Adaptive类.
​	例如 Protocol的spi类有 injvm dubbo registry filter listener等等 很多扩展未知类，
​	它设计了Protocol$Adaptive的类，通过ExtensionLoader.getExtensionLoader(Protocol.class).getExtension(spi类);来提取对象

RocketMQ:
​	RocketMQ所有消息都是持久化硬盘的。
​	p发送第一条消息给MQ。回调本地事物执行，保证MQ一定有数据，本地事物一定执行的。根据本地事物执行成功失败返回一个状态，执行成功的话就发送给MQ，MQ标识可以消费。
​	c就可以消费这个mq。
​	RocketMQ第一阶段发送Prepared(事先准备好的)消息时，会拿到消息的地址，第二阶段执行本地事物，第三阶段通过第一阶段拿到的地址
​	去访问消息，并修改状态。如果确认消息发送失败了怎么办？RocketMQ会定期扫描消息集群中的事物消息，如果发现了Prepared消息，
​	他会向消息发送者确认，通过RocketMQ策略来决定是回滚还是继续发送确认消息。


Zookeeper
​	使用Zookeeper的临时性ZNode来存放服务提供者的RMI地址，一旦与服务提供者的Session中断，自动清除ZNode。
​	服务消费者去监听ZNode，一旦发现Znode的数据有变化，就会从新获取一份有效数据的拷贝。
​	服务消费者在创建的时候连接Zookeeper，同时监听/register节点的NodeChildrenChanage事件，一旦/register节点的子节点变化，就需要重写获取最新的子节点。


分布式锁，幂等性、一致性
​	1)使用redis的setnx()、expire()方法，用于分布式锁
​		1. setnx(lockkey, 1)  如果返回0，则说明占位失败；如果返回1，则说明占位成功
​		2. expire()命令对lockkey设置超时时间，为的是避免死锁问题。
​		3. 执行完业务代码后，可以通过delete命令删除key。
​		  这个方案其实是可以解决日常工作中的需求的，但从技术方案的探讨上来说，可能还有一些可以完善的地方。比如，如果在第一步setnx执行成功后，在expire()命令执行成
​		  功前，发生了宕机的现象，那么就依然会出现死锁的问题，所以如果要对其进行完善的话，可以使用redis的setnx()、get()和getset()方法来实现分布式锁。   
​	2)基于数据库表做乐观锁，用于分布式锁。
​	3)队列处理

##### 分布式事物

​	1、更新数据库
​	2、更新成功发送消息，更新失败则不发送
​	3、发送消息失败则回滚

##### 	本地消息表实现：

​		核心思想是将分布式事务拆分为本地事务进行处理	

​		基本的思路：

​			消息生产方，需要额外建立一个消息表，并记录消息发送状态。消息表和业务数据表要在一个事务提交，他们是在一个数据库里面。然后消息会经过MQ发送到消息的消费方。如果消息发送失败，会进行重试发送

​			消息消费方，需要处理这个消息，并且完成自己的业务。此时如果本地事务处理成功，表明已经处理成功，如果处理失败，则会重试执行。如果是业务上的失败，可以给生产方发送一个业务补偿消息，通知生产方进行回滚等操作。

​			生产方和消费方定时扫描本地消息表，把还没处理完成或失败的消息再发送一遍。

​		优点：一种经典的实现，避免了分布式事务，实现了最终一致性。

​		缺点：消息表会和耦合到业务系统中。

##### 	MQ事务消息：

​		RocketMQ，他们支持事务消息的方式也是类似于采用的二阶段提交。

​		第一阶段Prepared消息，会拿到消息地址。第二阶段执行本地事务，第三阶段通过拿第一阶段地址去访问消息，并修改状态。

​		业务方法内想要消息队列提交两次请求，一次发送消息一次确认消息。如果确认消息发送失败RocketMQ会定期扫描消息集群的事务消息，这时候发现Prepared消息，会向消息发送者确认，所以生产方需要实现一个check接口，RocketMQ会根据发送端配置的策略来决定是回滚还是继续发送确认消息。这样就保证了消息发送与本地事务同时成功或失败。

##### 	![1554086754(1)](E:\develop\git_workspase\zhengjy-demo\zhengjy-demo-deploy\src\main\resources\file\md_img\1554086754(1).png)

RocketMQ分布式事物：
​	1、将扣费消息先注册到zk上，状态Init
​	2、执行更新数据库，
​	3、数据库更新成功，变更消息状态，exe
​	4、如果变更消息状态失败，rocketMQ会每分钟请求一次，注册到zk上的消息状态为init，询问是否作废还是重发

1)首先，MQ发送方向MQ服务发送半消息。
2)MQ服务端会将消息做持久化处理，并发送ACK确认消息已经发送成功。
3)MQ发送方执行本地事务
4)MQ发送方根据本地事务执行的结果向MQ服务提交二次确认:
​		如果本地事务执行成功，则提交消息状态为commit，否则rollback。MQ服务端收到commit状态的
​		消息标记为可投递状态，订阅方最终会收到该条消息。如果收到的是rollback，最终MQ服务端会
​		删除该条半消息，订阅方不会收到这条消息。
5)如果出现网络闪断、应用重启等情况，4阶段提交的二次确认最终并未能够到达MQ服务端，一定时间之后，
​	MQ服务端会对此消息发起回查操作，确认发送方本地事务的执行状态。
6)发送方需要实现服务回查逻辑MQ服务端进行回调。当发送方收到回查后，需要检查对应消息的本地事务
​	执行的最终结果，此外也需要根据本地事务的成功或失败返回commit或rollback，即在此提交消息
​	状态的二次确认，MQ服务端仍会按照步骤4对该半消息进行操作。
1-4 为事务消息的发送过程， 5-6 为事务消息的回查过程。





##### dubbo负载均衡

​	轮询算法
​		每一次把来自用户的请求轮流分配给内部中的服务器。如：从1开始，一直到N(其中，N是内部服务器的总个数)，然后重新开始循环。
​		优点：简洁性，无需记录当前所有连接状态，它是一种无状态调度。
​		缺点：轮询调用是假设所有服务器的处理性能相同，不关系每台服务器的连接数和响应速度。当请求服务间隔时间变化比较大时，轮询调度算法容易导致服务器间的负载不平衡。
​	一致性Hash
​		相同参数的请求总是发到同一个提供者。
​	随机算法
​		按权重设置随机概率
​	最少活跃调用数
​		相同活跃数的随机，活跃数指调用前后计数差。
​		使慢的提供者收到更少请求，因为越慢的提供者的调用前后计数差会越大。
​		
dubbo集群容错
​	Failover Cluster模式
​		这种模式是Dubbo集群容错默认的模式选择，调用失败时，会自动切换，重新尝试调用其他节点上可用的服务。
​	Failfast Cluster模式
​		快速失败模式，调用只执行一次，失败则立即报错。
​	Failback Cluster模式
​		配置值为failback。失败自动恢复，后台记录失败请求，定时重发。通常用于消息通知操作。
​	Forking Cluster模式
​		配置值为forking。并行调用多个服务器，只要一个成功即返回。通常用于实时性要求较高的读操作，但需要浪费更多服务资源。

#### 一致性哈希：

​	 1.首先求出每个节点(机器名或者是IP地址)的哈希值，并将其分配到一个圆环区间上（这里取0-2^32）。
​     2.求出需要存储对象的哈希值，也将其分配到这个圆环上。
​     3.从对象映射到的位置开始顺时针查找，将对象保存到找到的第一个节点上。
​	 考虑到哈希算法在node较少的情况下，改变node数会带来巨大的数据迁移。为了解决这种情况，一致性哈希引入了“虚拟节点”的概念： 
​	 “虚拟节点”是实际节点在环形空间的复制品，一个实际节点对应了若干个“虚拟节点”，“虚拟节点”在哈希空间中以哈希值排列。

​	 一致性哈希将整个哈希值空间组织成一个虚拟的圆环，如假设某哈希函数H的值空间为0 - 232-1（即哈希值是一个32位无符号整形），整个哈希空间环如下：

![2](E:\develop\git_workspase\zhengjy-demo\zhengjy-demo-deploy\src\main\resources\file\md_img\一致性哈希算法\2.png)



整个空间按顺时针方向组织。0和232-1在零点中方向重合。

下一步将各个服务器使用H进行一个哈希，具体可以选择服务器的ip或主机名作为关键字进行哈希，这样每台机器就能确定其在哈希环上的位置，这里假设将上文中三台服务器使用ip地址哈希后在环空间的位置如下：

![3](E:\develop\git_workspase\zhengjy-demo\zhengjy-demo-deploy\src\main\resources\file\md_img\一致性哈希算法\3.png)

接下来使用如下算法定位数据访问到相应服务器：将数据key使用相同的函数H计算出哈希值h，通根据h确定此数据在环上的位置，从此位置沿环顺时针“行走”，第一台遇到的服务器就是其应该定位到的服务器。

例如我们有A、B、C、D四个数据对象，经过哈希计算后，在环空间上的位置如下：

![4](E:\develop\git_workspase\zhengjy-demo\zhengjy-demo-deploy\src\main\resources\file\md_img\一致性哈希算法\4.png)

根据一致性哈希算法，数据A会被定为到Server 1上，D被定为到Server 3上，而B、C分别被定为到Server 2上。

##### 容错性与可扩展性分析

下面分析一致性哈希算法的容错性和可扩展性。现假设Server 3宕机了：

​	 ![5](E:\develop\git_workspase\zhengjy-demo\zhengjy-demo-deploy\src\main\resources\file\md_img\一致性哈希算法\5.png)



可以看到此时A、C、B不会受到影响，只有D节点被重定位到Server 2。一般的，在一致性哈希算法中，如果一台服务器不可用，则受影响的数据仅仅是此服务器到其环空间中前一台服务器（即顺着逆时针方向行走遇到的第一台服务器）之间数据，其它不会受到影响。

下面考虑另外一种情况，如果我们在系统中增加一台服务器Memcached Server 4：

![6](E:\develop\git_workspase\zhengjy-demo\zhengjy-demo-deploy\src\main\resources\file\md_img\一致性哈希算法\6.png)



此时A、D、C不受影响，只有B需要重定位到新的Server 4。一般的，在一致性哈希算法中，如果增加一台服务器，则受影响的数据仅仅是新服务器到其环空间中前一台服务器（即顺着逆时针方向行走遇到的第一台服务器）之间数据，其它不会受到影响。

综上所述，一致性哈希算法对于节点的增减都只需重定位环空间中的一小部分数据，具有较好的容错性和可扩展性。

##### 虚拟节点

一致性哈希算法在服务节点太少时，容易因为节点分部不均匀而造成数据倾斜问题。例如我们的系统中有两台服务器，其环分布如下：

![7](E:\develop\git_workspase\zhengjy-demo\zhengjy-demo-deploy\src\main\resources\file\md_img\一致性哈希算法\7.png)

此时必然造成大量数据集中到Server 1上，而只有极少量会定位到Server 2上。为了解决这种数据倾斜问题，一致性哈希算法引入了虚拟节点机制，即对每一个服务节点计算多个哈希，每个计算结果位置都放置一个此服务节点，称为虚拟节点。具体做法可以在服务器ip或主机名的后面增加编号来实现。例如上面的情况，我们决定为每台服务器计算三个虚拟节点，于是可以分别计算“Memcached Server 1#1”、“Memcached Server 1#2”、“Memcached Server 1#3”、“Memcached Server 2#1”、“Memcached Server 2#2”、“Memcached Server 2#3”的哈希值，于是形成六个虚拟节点：

![8](E:\develop\git_workspase\zhengjy-demo\zhengjy-demo-deploy\src\main\resources\file\md_img\一致性哈希算法\8.png)



同时数据定位算法不变，只是多了一步虚拟节点到实际节点的映射，例如定位到“Memcached Server 1#1”、“Memcached Server 1#2”、“Memcached Server 1#3”三个虚拟节点的数据均定位到Server 1上。这样就解决了服务节点少时数据倾斜的问题。在实际应用中，通常将虚拟节点数设置为32甚至更大，因此即使很少的服务节点也能做到相对均匀的数据分布。

#### 服务器性能压测：

​	TCP网络参数调优
​		标识一个TCP连接需要四个元素组成(源(客户端)ip、port,目标(服务端)ip、port。)。一台服务器一般有2的48次方(65536)个端口，
​		即客户端做压测时受限制最多只能打开2*48次方个端口。服务器则没有限制，但是服务端会有"文件句柄(打开文件的数量)的限制"。
​		客户端：
​			端口的范围扩大，/proc/sys/net/ipv4/ip_local_port_range
​		服务端：
​			文件句柄的增加，/etc/security/limits.conf
​		

 由于TCP是全双工的，因此关闭连接必须在两个方向上分别进行。首先发起关闭的一方为主动关闭方，另一方为被动关闭方。很多人都会在这里晕掉，
 实际上四次挥手比三次握手还简单。四次挥手简单地分为三个过程：
​		过程一.主动关闭方发送FIN，被动关闭方收到后发送该FIN的ACK；
​		过程二.被动关闭方发送FIN，主动关闭方收到后发送该FIN的ACK；
​		过程三.被动关闭方收到ACK。
以上三步下来，圆圈就闭合了！也就是说，在过程三后，被动关闭方就可以100%确认连接已经关闭，因此它便可以直接进入CLOSE状态了，
然而主动关闭的一方，它无法确定最后的那个发给被动关闭方的ACK是否已经被收到，据TCP协议规范，不对ACK进行ACK，因此它不可能再收到被动关闭方的
任何数据了，因此在这里就陷入了僵局，TCP连接的主动关闭方如何来保证圆圈的闭合？这里，协议外的东西起作用了，和STP(Spanning tree)依靠各类超
时值来收敛一样，IP也有一个超时值，即MSL。
主动关闭一方等待MSL时间再释放连接，这个状态就是TIME_WAIT。对于被动关闭的一方，发出FIN之后就处在了LAST_ACK状态了，既然已经发出FIN了，缺的无
非也就是个ACK，连接本身其实已经关闭了，因此被动关闭的一方就没有TIME_WAIT状态。
​	





#### ZK Leader选举细节

​	当有一台服务器(假设此机器myid为1,名称为server1)启动时，它是无法Leader选举的，当第二太机器(myid为2，名称为server2)也启动，此时两台机器可以互相通讯，每台机器都试图找
​	到一个Leader，便进入Leader选举流程。
​	

	1)每个Server会发出一个投票
		初始状态，server1和server2都会将自己作为Leader进行投票，每次投票包含服务器的myid和ZXID，以(myid,ZXID)来表示，初始状态，server1和server2都会投给自己，
		即server投票为(1,0),server2投票为(2,0),然后各自将这个投票发给集群中其他机器。
	2)接收各个服务器的投票
		每个服务器都会接收来自其他服务器的投票。集群中的每个服务器在接收到投票后，首先会判断该投票的有效性，包括检查是否是本轮投票，是否来自LOOKING状态的服务器。
	3)处理投票
		在接收来自其他服务器的投票后，针对每个投票，服务器都需要将别人的投票和自己的投票进行PK，规则如下；
			3.1) 优先检查ZXID，ZXID比较大的服务器优先作为Leader
			3.2)如果ZXID相同的话，那么比较myid。myid比较大的服务器作为Leader
		对应server1投票是(1,0),而接收到的投票为(2,0)。首先会比对ZXID，因为都是0,所以无法决定谁是Leader。接下来比对两者的myid,server1发现接收到的投票中myid是2，大于自己
		于是就会更新自己的投票为(2,0),然后重新将投票发送出去。对server2来说，不需要更新自己的投票信息，只是再一次向集群机器发送上一次投票信息即可。
	4)统计投票
		每次投票后，服务器都会统计所有投票，判断是否有过半的机器接收到相同的投票信息，对于server1和server2服务器来说，都统计出集群中已经有两天机器接受了(2,0)这个投票
		信息，即认为选出了Leader。
	5)改变服务器状态
		一旦确认了Leader，每个服务器就会更新自己状态：如果是Follower，那么就变更为Following，如果是Leader，则变更为Leading。

运行期间的Leader选举
​	假设zk服务器由3台机器组成，分别为server1、server2、server3，当前Leader是server2，假设Leader挂了，便开始Leader选举
​	
​	1)变更状态
​		当Leader挂了之后，余下的非Observer服务器都会将自己的服务器状态变更为LOOKING，然后进行Leader选举流程。
​	2)每个Server都会发出一个投票
​		在这个过程中，需要生成投票信息(myid,ZXID)。因为是运行期间，因此每个服务器上的ZXID可能不同，假定Server1的ZXID为123，Server3的ZXID为122。在第一轮的投票中，server1
​		和server3都会投给自己，即分别产生投票(1,123),和(3,122),然后各自将这个投票发给集群中其他机器。
​	3)接收来自各个服务器的投票
​	4)处理投票
​		对于投票的处理，这个例子里面，由于server1的ZXID为123，server3的ZXID为122，所以server1会成为Leader。



### ZAB协议

ZAB主备模式下保持各个副本 之间的数据一致性。崩溃恢复过程保证机器的高可用。

如果是读请求直接从当前节点读取数据。如果写请求且当前不是leader，那么节点就会像leader提交事务，leader会广播事务，只有有超过半数节点成功，该请求就会被提交。

##### zxid

![20](E:\develop\git_workspase\zhengjy-demo\zhengjy-demo-deploy\src\main\resources\file\md_img\zab\20.png)

zxid是一个64位数字，分为低32位、高32位：

​	低32位是一个递增计数器，针对客户端每个事务请求，计数器+1；

​	高32位是Leader周期的epoch的编号，即是每次选举一次新的leader就+1。每当产生新的Leader服务器，就会从Leader服务器上取出本地日志中最大（zxid）事务id，并从中读取epoch值，然后+1，以此作为新的epoch，并将低32位从0开始计数。

##### myid

每个ZooKeeper服务器，都需要在数据文件夹下创建一个名为myid的文件，该文件包含整个ZooKeeper集群唯一的ID（整数）。例如，某ZooKeeper集群包含三台服务器，hostname分别为zoo1、zoo2和zoo3，其myid分别为1、2和3，则在配置文件中其ID与hostname必须一一对应，如下所示。在该配置文件中，server.后面的数据即为myid

server.1=zoo1:2888:3888

server.2=zoo2:2888:3888

server.3=zoo3:2888:3888

##### **logicClock** 

每个服务器会维护一个自增的整数，名为logicClock，它表示这是该服务器发起的第多少轮投票

##### **服务器状态**

- **LOOKING** 不确定Leader状态。该状态下的服务器认为当前集群中没有Leader，会发起Leader选举。
- **FOLLOWING** 跟随者状态。表明当前服务器角色是Follower，并且它知道Leader是谁。
- **LEADING** 领导者状态。表明当前服务器角色是Leader，它会维护与Follower间的心跳。
- **OBSERVING** 观察者状态。表明当前服务器角色是Observer，与Folower唯一的不同在于不参与选举，也不参与集群写操作时的投票。

#### **原子广播（ZAB）**

##### 写请求

![3](E:\develop\git_workspase\zhengjy-demo\zhengjy-demo-deploy\src\main\resources\file\md_img\zab\3.png)

​	Leader写操作，分五步：

​	1、客户端向Leader发送请求

​	2、Leader将写请求以Proposal的形式发送给所有Follower并等到ACK

​	3、Follower收到Leader的Proposal后返回ACK

​	4、**Leader得到超过半数的ACK**(Leader对自己默认有一个ACK)后向所有的Follower和Observer发送Commit

​	5、Leader处理完结果返回给客户端

##### 写Follower/Observer

![4](E:\develop\git_workspase\zhengjy-demo\zhengjy-demo-deploy\src\main\resources\file\md_img\zab\4.png)

1、Follower/Observer均可接受写请求，但不能直接处理，而需要将写请求转发给Leader处理

2、除了多了一步请求转发，其它流程与直接写Leader无任何区别

##### 读操作

Leader/Follower/Observer都可直接处理读请求，从本地内存中读取数据并返回给客户端即可。

![5](E:\develop\git_workspase\zhengjy-demo\zhengjy-demo-deploy\src\main\resources\file\md_img\zab\5.png)

由于处理读请求不需要服务器之间的交互，Follower/Observer越多，整体可处理的读请求量越大，也即读性能越好。



#### 集群启动选举Leader

##### 初始投票给自己

集群刚启动时，所有服务器的logicClock都为1，zxid为0。各服务器初始化后，都投票给自己，并将自己的一票存入自己的票箱：

![6](E:\develop\git_workspase\zhengjy-demo\zhengjy-demo-deploy\src\main\resources\file\md_img\zab\6.png)

(1,1,0)第一位代表投出该选票的服务器的logicClock,第二位代表被推荐服务器的myid，第三位代表被推荐的服务器的最大的zxid。由于该步骤中所有选票都投给自己，所以第二位的myid即是自己的myid，第三位的zxid即是自己的zxid。此时各自的票箱中只有自己投给自己的一票。

##### 更新选票

服务器收到外部投票后，进行选票PK，相应更新自己的选票并广播出去，并将合适的选票存入自己的票箱

![7](E:\develop\git_workspase\zhengjy-demo\zhengjy-demo-deploy\src\main\resources\file\md_img\zab\7.png)

服务器1收到服务器2的选票(1,2,0)和服务器3的选票(1,3,0)后，由于所有的logicClock都相等和所有的zxid都相等，因此根据myid判断应该将自己的选票按照
服务器3的选票更新为(1,3,0),并将自己的选票箱全部清空，再讲服务器3的选票与自己的选票存入自己的票箱，接着将自己更新后的选票广播出去。
此时服务器1票箱内的选票为(1,3),(3,3)。

同理，服务器2收到服务器3的选票后也将自己的选票更新为(1,3,0)并存入票箱然后广播，此时服务器2票箱内的选票为(2,3),(3,3)。

服务3根据上述规则，无需更新选票,自身的票箱扔为(3,3)。

服务器1与服务器2更新后的选票广播出去后，由于三个服务器最新选票都相同，最后三者的票箱内部都包含三张投给服务器3的选票。

##### 更新选票确定角色

根据上述选票，三个服务器一致认为此时服务器3应该是Leader。因此服务器1和2都进入FOLLOWING状态，而服务器3进入LEADING状态。之后Leader发起并维护与Follower间的心跳。

![8](E:\develop\git_workspase\zhengjy-demo\zhengjy-demo-deploy\src\main\resources\file\md_img\zab\8.png)

#### Follower重启选票

##### Follower重启投票给自己

Follower重启，或者发生网络分区后找不到Leader，会进入LOOKING状态并发起新的一轮投票。

![9](E:\develop\git_workspase\zhengjy-demo\zhengjy-demo-deploy\src\main\resources\file\md_img\zab\9.png)

##### 发现已有Leader后成为Follower

服务器3收到服务器1的投票后，将自己的状态Leading以及选票返回给服务器1。服务器2收到服务器1的投票后，将自己的状态Following及选票返回给服务器1。
此时服务器1知道服务器3是Leader，并且通过服务器2与服务器3的选票可以确定服务器3确实得到超过半数的选票，因此服务器进入Following状态。

![10](E:\develop\git_workspase\zhengjy-demo\zhengjy-demo-deploy\src\main\resources\file\md_img\zab\10.png)

#### Leader重启选举

##### Follower发起新投票

Leader（服务器3）宕机后，Follower（服务器1和2）发现Leader不工作了，因此进入LOOKING状态并发起新的一轮投票，并且都将票投给自己。

![11](E:\develop\git_workspase\zhengjy-demo\zhengjy-demo-deploy\src\main\resources\file\md_img\zab\11.png)

##### 广播更新选票

在上图中，服务器1的zxid为11，而服务器2的zxid为10，因此服务器2将自身选票更新为（3, 1, 11），如下图所示。

![12](E:\develop\git_workspase\zhengjy-demo\zhengjy-demo-deploy\src\main\resources\file\md_img\zab\12.png)

##### 选举新Leader

经过上一步选票更新后，服务器1与服务器2均将选票投给服务器1，因此服务器2成为Follower，而服务器1成为新的Leader并维护与服务器2的心跳。

![13](E:\develop\git_workspase\zhengjy-demo\zhengjy-demo-deploy\src\main\resources\file\md_img\zab\13.png)

##### 旧Leader恢复后发起选举

旧的Leader恢复后，进入Looking状态并发起新一轮领导选举，并将选票投给自己。此时服务器1会根据自己的leading状态及选票(3,1,11)返回给服务器3，而服务器2将自己的Following状态及选票(3,1,11)返回给服务3。如下图所示。

![14](E:\develop\git_workspase\zhengjy-demo\zhengjy-demo-deploy\src\main\resources\file\md_img\zab\14.png)

##### 旧Leader成为Follower

服务器3了解到Leader为服务器1，且根据选票了解到服务器1确实得到过半服务器的选票，因此自己进入FOLLOWING状态。

![15](E:\develop\git_workspase\zhengjy-demo\zhengjy-demo-deploy\src\main\resources\file\md_img\zab\15.png)

#### **Commit过的数据不丢失**

##### Failover前状态

为更好演示Leader Failover过程，本例中共使用5个ZooKeeper服务器。A作为Leader，共收到P1、P2、P3三条消息，并且Commit了1和2，且总体顺序为P1、P2、C1、P3、C2。根据顺序性原则，其它Follower收到的消息的顺序肯定与之相同。其中B与A完全同步，C收到P1、P2、C1，D收到P1、P2，E收到P1，如下图所示。

![16](E:\develop\git_workspase\zhengjy-demo\zhengjy-demo-deploy\src\main\resources\file\md_img\zab\16.png)

这里要注意：

- 由于A没有C3，意味着收到P3的服务器的总个数不会超过一半，也即包含A在内最多只有两台服务器收到P3。在这里A和B收到P3，其它服务器均未收到P3
- 由于A已写入C1、C2，说明它已经Commit了P1、P2，因此整个集群有超过一半的服务器，即最少三个服务器收到P1、P2。在这里所有服务器都收到了P1，除E外其它服务器也都收到了P2

#####  选举新Leader

旧Leader也即A宕机后，其它服务器根据上述FastLeaderElection算法选出B作为新的Leader。C、D和E成为Follower且以B为Leader后，会主动将自己最大的zxid发送给B，B会将Follower的zxid与自身zxid间的所有被Commit过的消息同步给Follower，如下图所示。

![17](E:\develop\git_workspase\zhengjy-demo\zhengjy-demo-deploy\src\main\resources\file\md_img\zab\17.png)

在上图中：

- P1和P2都被A Commit，因此B会通过同步保证P1、P2、C1与C2都存在于C、D和E中
- P3由于未被A Commit，同时幸存的所有服务器中P3未存在于大多数据服务器中，因此它不会被同步到其它Follower

##### 通知Follower可对外服务

同步完数据后，B会向D、C和E发送NEWLEADER命令并等待大多数服务器的ACK（下图中D和E已返回ACK，加上B自身，已经占集群的大多数），然后向所有服务器广播UPTODATE命令。收到该命令后的服务器即可对外提供服务。

![18](E:\develop\git_workspase\zhengjy-demo\zhengjy-demo-deploy\src\main\resources\file\md_img\zab\18.png)

#### 未Commit过的消息对客户端不可见

在上例中，P3未被A Commit过，同时因为没有过半的服务器收到P3，因此B也未Commit P3（如果有过半服务器收到P3，即使A未Commit P3，B会主动Commit P3，即C3），所以它不会将P3广播出去。

 

具体做法是，B在成为Leader后，先判断自身未Commit的消息（本例中即P3）是否存在于大多数服务器中从而决定是否要将其Commit。然后B可得出自身所包含的被Commit过的消息中的最小zxid（记为min_zxid）与最大zxid（记为max_zxid）。C、D和E向B发送自身Commit过的最大消息zxid（记为max_zxid）以及未被Commit过的所有消息（记为zxid_set）。B根据这些信息作出如下操作：

 

- 如果Follower的max_zxid与Leader的max_zxid相等，说明该Follower与Leader完全同步，无须同步任何数据
- 如果Follower的max_zxid在Leader的(min_zxid，max_zxid)范围内，Leader会通过TRUNC命令通知Follower将其zxid_set中大于Follower的max_zxid（如果有）的所有消息全部删除

上述操作保证了未被Commit过的消息不会被Commit从而对外不可见。



 上述例子中Follower上并不存在未被Commit的消息。但可考虑这种情况，如果将上述例子中的服务器数量从五增加到七，服务器F包含P1、P2、C1、P3，服务器G包含P1、P2。此时服务器F、A和B都包含P3，但是因为票数未过半，因此B作为Leader不会Commit P3，而会通过TRUNC命令通知F删除P3。如下图所示。

![19](E:\develop\git_workspase\zhengjy-demo\zhengjy-demo-deploy\src\main\resources\file\md_img\zab\19.png)

- 由于使用主从复制模式，所有的写操作都要由Leader主导完成，而读操作可通过任意节点完成，因此ZooKeeper读性能远好于写性能，更适合读多写少的场景
- 虽然使用主从复制模式，同一时间只有一个Leader，但是Failover机制保证了集群不存在单点失败（SPOF）的问题
- ZAB协议保证了Failover过程中的数据一致性
- 服务器收到数据后先写本地文件再进行处理，保证了数据的持久性



![1](E:\develop\git_workspase\zhengjy-demo\zhengjy-demo-deploy\src\main\resources\file\md_img\zab\1.png)

![2](E:\develop\git_workspase\zhengjy-demo\zhengjy-demo-deploy\src\main\resources\file\md_img\zab\2.png)



比较zxid

①如果内部投票的zxid大于收到的外部投票，忽略当前外部投票

②如果内部投票的zxid小于收到的外部投票，变更自己的投票，并广播给其他节点

③如果二者相等，比较myid

比较myid

①如果内部投票的myid大于收到的外部投票，忽略当前外部投票

②如果内部投票的myid小于收到的外部投票，变更自己的投票，并广播给其他节点




数据库成倍扩容 






















​	
​	
​	
​	
​	
​	